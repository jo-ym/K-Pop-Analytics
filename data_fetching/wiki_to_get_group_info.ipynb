{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def get_decade_links(url):\n",
    "    \"\"\"\n",
    "    Retrieve links to different decades from a given Wikipedia page.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the Wikipedia page.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are decades (e.g., '1990s') and values are their corresponding URLs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send an HTTP GET request to the provided URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Initialize a dictionary to store the results\n",
    "        decade_links = {}\n",
    "        \n",
    "        # Find all div elements with the class 'hatnote' (contains \"Main article\")\n",
    "        main_articles = soup.find_all('div', class_='hatnote')\n",
    "        for article in main_articles:\n",
    "            link = article.find('a', href=True)  # Find the first anchor tag with an href attribute\n",
    "            if link:\n",
    "                # Extract the text and URL from the link\n",
    "                decade_text = link.text.strip()\n",
    "                \n",
    "                # Extract only the decade part (e.g., '1990s', '2000s')\n",
    "                decade_match = re.search(r'\\b\\d{4}s\\b', decade_text)\n",
    "                if decade_match:\n",
    "                    decade = decade_match.group()\n",
    "                    full_url = f\"https://en.wikipedia.org{link['href']}\"\n",
    "                    decade_links[decade] = full_url  # Add the decade and URL to the dictionary\n",
    "        \n",
    "        return decade_links\n",
    "    except requests.RequestException as e:\n",
    "        # Handle HTTP-related errors\n",
    "        print(f\"Error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1990s': 'https://en.wikipedia.org/wiki/List_of_South_Korean_idol_groups_(1990s)',\n",
       " '2000s': 'https://en.wikipedia.org/wiki/List_of_South_Korean_idol_groups_(2000s)',\n",
       " '2010s': 'https://en.wikipedia.org/wiki/List_of_South_Korean_idol_groups_(2010s)',\n",
       " '2020s': 'https://en.wikipedia.org/wiki/List_of_South_Korean_idol_groups_(2020s)'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_South_Korean_idol_groups'\n",
    "decade_links_info = get_decade_links(url)\n",
    "decade_links_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_footnotes(text):\n",
    "    \"\"\"\n",
    "    Removes footnotes and bracketed content (e.g., [1], (info)) from the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to clean.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned text without footnotes or bracketed content.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text  # Return as is if text is None or empty\n",
    "    # Remove [content] and (content)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove square brackets and content within\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)  # Remove parentheses and content within\n",
    "    return text.strip()  # Trim any extra whitespace\n",
    "\n",
    "\n",
    "def parse_kpop_groups_cleaned(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parses K-pop idol groups by year from a Wikipedia page, removing footnotes and brackets.\n",
    "    \n",
    "    Args:\n",
    "        url (str): Wikipedia page URL.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing 'start_year', 'group', and 'wiki_url'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        records = []\n",
    "        headings = soup.find_all(['h2', 'div'], class_='mw-heading mw-heading2')\n",
    "        for heading in headings:\n",
    "            year = None\n",
    "            if heading.name == 'h2':  # Case: <h2 id=\"2010\">2010</h2>\n",
    "                year = heading.get('id')\n",
    "            elif heading.name == 'div':  # Case: <div class=\"mw-heading mw-heading2\"><h2 id=\"2000\">2000</h2>\n",
    "                h2_tag = heading.find('h2')\n",
    "                if h2_tag:\n",
    "                    year = h2_tag.get('id')\n",
    "            \n",
    "            if not year or not year.isdigit():\n",
    "                continue\n",
    "            \n",
    "            sibling = heading.find_next_sibling()\n",
    "            while sibling:\n",
    "                if sibling.name in ['h2', 'div'] and 'mw-heading' in sibling.get('class', ''):\n",
    "                    break\n",
    "                \n",
    "                if sibling.name == 'ul':  # Groups are often listed in <ul>\n",
    "                    for li in sibling.find_all('li'):\n",
    "                        group_name = li.text.strip()\n",
    "                        group_name = clean_text_footnotes(group_name)  # Clean footnotes\n",
    "                        link_tag = li.find('a', href=True)\n",
    "                        wiki_url = f\"https://en.wikipedia.org{link_tag['href']}\" if link_tag else None\n",
    "                        # Skip empty group names\n",
    "                        if group_name:\n",
    "                            records.append({'start_year': year, 'group': group_name, 'wiki_url': wiki_url})\n",
    "                elif sibling.name == 'div' and 'div-col' in sibling.get('class', []):  # Multi-column layout\n",
    "                    for li in sibling.find_all('li'):\n",
    "                        group_name = li.text.strip()\n",
    "                        group_name = clean_text_footnotes(group_name)  # Clean footnotes\n",
    "                        link_tag = li.find('a', href=True)\n",
    "                        wiki_url = f\"https://en.wikipedia.org{link_tag['href']}\" if link_tag else None\n",
    "                        # Skip empty group names\n",
    "                        if group_name:\n",
    "                            records.append({'start_year': year, 'group': group_name, 'wiki_url': wiki_url})\n",
    "                \n",
    "                sibling = sibling.find_next_sibling()\n",
    "        \n",
    "        return pd.DataFrame(records)\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the page: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the page: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_year</th>\n",
       "      <th>group</th>\n",
       "      <th>wiki_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>Coed School</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Coed_School</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>DMTN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/DMTN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>F.Cuz</td>\n",
       "      <td>https://en.wikipedia.org/wiki/F.Cuz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>Girl's Day</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Girl%27s_Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>GD &amp; TOP</td>\n",
       "      <td>https://en.wikipedia.org/wiki/GD_%26_TOP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>2019</td>\n",
       "      <td>Vanner</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Vanner_(band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>2019</td>\n",
       "      <td>Verivery</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Verivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>2019</td>\n",
       "      <td>We in the Zone</td>\n",
       "      <td>https://en.wikipedia.org/wiki/We_in_the_Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>2019</td>\n",
       "      <td>Wooseok x Kuanlin</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wooseok_x_Kuanlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>2019</td>\n",
       "      <td>X1</td>\n",
       "      <td>https://en.wikipedia.org/wiki/X1_(band)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    start_year              group  \\\n",
       "0         2010        Coed School   \n",
       "1         2010               DMTN   \n",
       "2         2010              F.Cuz   \n",
       "3         2010         Girl's Day   \n",
       "4         2010           GD & TOP   \n",
       "..         ...                ...   \n",
       "309       2019             Vanner   \n",
       "310       2019           Verivery   \n",
       "311       2019     We in the Zone   \n",
       "312       2019  Wooseok x Kuanlin   \n",
       "313       2019                 X1   \n",
       "\n",
       "                                            wiki_url  \n",
       "0          https://en.wikipedia.org/wiki/Coed_School  \n",
       "1                 https://en.wikipedia.org/wiki/DMTN  \n",
       "2                https://en.wikipedia.org/wiki/F.Cuz  \n",
       "3         https://en.wikipedia.org/wiki/Girl%27s_Day  \n",
       "4           https://en.wikipedia.org/wiki/GD_%26_TOP  \n",
       "..                                               ...  \n",
       "309      https://en.wikipedia.org/wiki/Vanner_(band)  \n",
       "310           https://en.wikipedia.org/wiki/Verivery  \n",
       "311     https://en.wikipedia.org/wiki/We_in_the_Zone  \n",
       "312  https://en.wikipedia.org/wiki/Wooseok_x_Kuanlin  \n",
       "313          https://en.wikipedia.org/wiki/X1_(band)  \n",
       "\n",
       "[314 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_South_Korean_idol_groups_(2010s)'\n",
    "titles = parse_kpop_groups_cleaned(url)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1990s from https://en.wikipedia.org/wiki/List_of_South_Korean_idol_groups_(1990s)...\n",
      "Processing 2000s from https://en.wikipedia.org/wiki/List_of_South_Korean_idol_groups_(2000s)...\n",
      "Processing 2010s from https://en.wikipedia.org/wiki/List_of_South_Korean_idol_groups_(2010s)...\n",
      "Processing 2020s from https://en.wikipedia.org/wiki/List_of_South_Korean_idol_groups_(2020s)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_year</th>\n",
       "      <th>group</th>\n",
       "      <th>wiki_url</th>\n",
       "      <th>decade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992</td>\n",
       "      <td>Seo Taiji and Boys</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Seo_Taiji_and_Boys</td>\n",
       "      <td>1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993</td>\n",
       "      <td>Deux</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Deux_(band)</td>\n",
       "      <td>1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1994</td>\n",
       "      <td>Cool</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cool_(band)</td>\n",
       "      <td>1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994</td>\n",
       "      <td>Roo'ra</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Roo%27ra</td>\n",
       "      <td>1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994</td>\n",
       "      <td>Two Two</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Two_Two</td>\n",
       "      <td>1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>2024</td>\n",
       "      <td>Say My Name</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Say_My_Name_(group)</td>\n",
       "      <td>2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>2024</td>\n",
       "      <td>TWS</td>\n",
       "      <td>https://en.wikipedia.org/wiki/TWS_(group)</td>\n",
       "      <td>2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>2024</td>\n",
       "      <td>Unis</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Unis_(group)</td>\n",
       "      <td>2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>2024</td>\n",
       "      <td>Waker</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Waker</td>\n",
       "      <td>2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>2024</td>\n",
       "      <td>Waterfire</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Waterfire_(band)</td>\n",
       "      <td>2020s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>541 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    start_year               group  \\\n",
       "0         1992  Seo Taiji and Boys   \n",
       "1         1993                Deux   \n",
       "2         1994                Cool   \n",
       "3         1994              Roo'ra   \n",
       "4         1994             Two Two   \n",
       "..         ...                 ...   \n",
       "536       2024         Say My Name   \n",
       "537       2024                 TWS   \n",
       "538       2024                Unis   \n",
       "539       2024               Waker   \n",
       "540       2024           Waterfire   \n",
       "\n",
       "                                              wiki_url decade  \n",
       "0     https://en.wikipedia.org/wiki/Seo_Taiji_and_Boys  1990s  \n",
       "1            https://en.wikipedia.org/wiki/Deux_(band)  1990s  \n",
       "2            https://en.wikipedia.org/wiki/Cool_(band)  1990s  \n",
       "3               https://en.wikipedia.org/wiki/Roo%27ra  1990s  \n",
       "4                https://en.wikipedia.org/wiki/Two_Two  1990s  \n",
       "..                                                 ...    ...  \n",
       "536  https://en.wikipedia.org/wiki/Say_My_Name_(group)  2020s  \n",
       "537          https://en.wikipedia.org/wiki/TWS_(group)  2020s  \n",
       "538         https://en.wikipedia.org/wiki/Unis_(group)  2020s  \n",
       "539                https://en.wikipedia.org/wiki/Waker  2020s  \n",
       "540     https://en.wikipedia.org/wiki/Waterfire_(band)  2020s  \n",
       "\n",
       "[541 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_decades_df = pd.DataFrame()\n",
    "for decade, url in decade_links_info.items():\n",
    "    print(f\"Processing {decade} from {url}...\")\n",
    "    \n",
    "    # Get group name of this decase\n",
    "    decade_df = parse_kpop_groups_cleaned(url)\n",
    "    \n",
    "    if not decade_df.empty:\n",
    "        decade_df['decade'] = decade\n",
    "        all_decades_df = pd.concat([all_decades_df, decade_df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"No data found for {decade}.\")\n",
    "all_decades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_year</th>\n",
       "      <th>group</th>\n",
       "      <th>wiki_url</th>\n",
       "      <th>decade</th>\n",
       "      <th>group_wiki_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992</td>\n",
       "      <td>Seo Taiji and Boys</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Seo_Taiji_and_Boys</td>\n",
       "      <td>1990s</td>\n",
       "      <td>Seo_Taiji_and_Boys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993</td>\n",
       "      <td>Deux</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Deux_(band)</td>\n",
       "      <td>1990s</td>\n",
       "      <td>Deux_(band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1994</td>\n",
       "      <td>Cool</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cool_(band)</td>\n",
       "      <td>1990s</td>\n",
       "      <td>Cool_(band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994</td>\n",
       "      <td>Roo'ra</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Roo%27ra</td>\n",
       "      <td>1990s</td>\n",
       "      <td>Roo'ra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994</td>\n",
       "      <td>Two Two</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Two_Two</td>\n",
       "      <td>1990s</td>\n",
       "      <td>Two_Two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>2024</td>\n",
       "      <td>Say My Name</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Say_My_Name_(group)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Say_My_Name_(group)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>2024</td>\n",
       "      <td>TWS</td>\n",
       "      <td>https://en.wikipedia.org/wiki/TWS_(group)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>TWS_(group)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>2024</td>\n",
       "      <td>Unis</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Unis_(group)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Unis_(group)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>2024</td>\n",
       "      <td>Waker</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Waker</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Waker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>2024</td>\n",
       "      <td>Waterfire</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Waterfire_(band)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Waterfire_(band)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>541 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    start_year               group  \\\n",
       "0         1992  Seo Taiji and Boys   \n",
       "1         1993                Deux   \n",
       "2         1994                Cool   \n",
       "3         1994              Roo'ra   \n",
       "4         1994             Two Two   \n",
       "..         ...                 ...   \n",
       "536       2024         Say My Name   \n",
       "537       2024                 TWS   \n",
       "538       2024                Unis   \n",
       "539       2024               Waker   \n",
       "540       2024           Waterfire   \n",
       "\n",
       "                                              wiki_url decade  \\\n",
       "0     https://en.wikipedia.org/wiki/Seo_Taiji_and_Boys  1990s   \n",
       "1            https://en.wikipedia.org/wiki/Deux_(band)  1990s   \n",
       "2            https://en.wikipedia.org/wiki/Cool_(band)  1990s   \n",
       "3               https://en.wikipedia.org/wiki/Roo%27ra  1990s   \n",
       "4                https://en.wikipedia.org/wiki/Two_Two  1990s   \n",
       "..                                                 ...    ...   \n",
       "536  https://en.wikipedia.org/wiki/Say_My_Name_(group)  2020s   \n",
       "537          https://en.wikipedia.org/wiki/TWS_(group)  2020s   \n",
       "538         https://en.wikipedia.org/wiki/Unis_(group)  2020s   \n",
       "539                https://en.wikipedia.org/wiki/Waker  2020s   \n",
       "540     https://en.wikipedia.org/wiki/Waterfire_(band)  2020s   \n",
       "\n",
       "         group_wiki_name  \n",
       "0     Seo_Taiji_and_Boys  \n",
       "1            Deux_(band)  \n",
       "2            Cool_(band)  \n",
       "3                 Roo'ra  \n",
       "4                Two_Two  \n",
       "..                   ...  \n",
       "536  Say_My_Name_(group)  \n",
       "537          TWS_(group)  \n",
       "538         Unis_(group)  \n",
       "539                Waker  \n",
       "540     Waterfire_(band)  \n",
       "\n",
       "[541 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import unquote\n",
    "\n",
    "all_decades_df['group_wiki_name'] = all_decades_df['wiki_url'].apply(lambda url: unquote(url.split('/')[-1]))\n",
    "all_decades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decades_df['start_year'] = all_decades_df['start_year'].apply(lambda x:int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decades_df.to_csv('./data/all_kpop_group_name_and_page.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_group_background(page_title: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch the background information of a K-pop group from a Wikipedia page Infobox using the MediaWiki API.\n",
    "    Handling redirects to ensure correct page. Processes multiple active year ranges correctly.\n",
    "    \n",
    "    Args:\n",
    "        page_title (str): The title of the Wikipedia page for the group.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dict containing 'start_year', 'end_year', 'company', 'hanja', 'hangul', and 'members'.\n",
    "            If no data is found, an empty dict is returned.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the API response or Infobox structure is unexpected.\n",
    "    \"\"\"\n",
    "    base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "    \n",
    "    # Resolve redirects to find the actual page title\n",
    "    query_params = {\n",
    "        'action': 'query',\n",
    "        'titles': page_title,\n",
    "        'redirects': 1,  # Automatically resolve redirects\n",
    "        'format': 'json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        query_response = requests.get(base_url, params=query_params)\n",
    "        query_response.raise_for_status()\n",
    "        query_data = query_response.json()\n",
    "        \n",
    "        # Extract the normalized title from the query response\n",
    "        pages = query_data['query']['pages']\n",
    "        page_id, page_info = next(iter(pages.items()))\n",
    "        if page_id == '-1':\n",
    "            raise ValueError(f\"Page '{page_title}' not found.\")\n",
    "        resolved_title = page_info.get('title', page_title)  # Fallback to original title if no redirect\n",
    "        \n",
    "        # Fetch and parse the page using the resolved title\n",
    "        parse_params = {\n",
    "            'action': 'parse',\n",
    "            'page': resolved_title,\n",
    "            'prop': 'text',\n",
    "            'format': 'json'\n",
    "        }\n",
    "        response = requests.get(base_url, params=parse_params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Parse the returned HTML content\n",
    "        html_content = data['parse']['text']['*']\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        infobox = soup.find('table', class_='infobox')\n",
    "        if not infobox:\n",
    "            raise ValueError(\"No infobox found on the page.\")\n",
    "        \n",
    "        # Initialize a dictionary to store group information\n",
    "        group_info = {\n",
    "            'start_year': None,\n",
    "            'end_year': None,\n",
    "            'company': None,\n",
    "            'hanja': None,\n",
    "            'hangul': None,\n",
    "            'members': None\n",
    "        }\n",
    "        \n",
    "        # Extract years active\n",
    "        years_row = infobox.find('th', string=re.compile(r'Years active', re.IGNORECASE))\n",
    "        if years_row:\n",
    "            years_data = years_row.find_next_sibling('td')\n",
    "            if years_data:\n",
    "                # Handle multiple ranges (e.g., <ul><li>2007–2017</li><li>2021–present</li></ul>)\n",
    "                year_ranges = []\n",
    "                for li in years_data.find_all('li'):\n",
    "                    year_text = li.get_text(strip=True)\n",
    "                    match = re.match(r\"(\\d{4})(?:\\s*(?:–|-|to)\\s*(\\d{4}|present))?\", year_text)\n",
    "                    if match:\n",
    "                        \n",
    "                        start = int(match.group(1))\n",
    "                        end = match.group(2)\n",
    "                        end = int(end) if end and end.isdigit() else None\n",
    "                        year_ranges.append((start, end))\n",
    "                \n",
    "                # Fallback for single range (if no <li>)\n",
    "                if not year_ranges:\n",
    "                    year_text = years_data.get_text(strip=True)\n",
    "                    match = re.match(r\"(\\d{4})(?:\\s*(?:–|-|to)\\s*(\\d{4}|present))?\", year_text)\n",
    "                    if match:\n",
    "                        start = int(match.group(1))\n",
    "                        end = match.group(2)\n",
    "                        end = int(end) if end and end.isdigit() else None\n",
    "                        year_ranges.append((start, end))\n",
    "                \n",
    "                # Determine overall start and end years\n",
    "                if year_ranges:\n",
    "                    group_info['start_year'] = min(start for start, _ in year_ranges)\n",
    "                    group_info['end_year'] = year_ranges[-1][-1]\n",
    "        \n",
    "        # Extract first label (company)\n",
    "        label_row = infobox.find('th', string=re.compile(r'Labels', re.IGNORECASE))\n",
    "        if label_row:\n",
    "            label_data = label_row.find_next_sibling('td')\n",
    "            if label_data:\n",
    "                first_label = label_data.find('a')\n",
    "                if first_label:\n",
    "                    group_info['company'] = ''.join(clean_text_footnotes(first_label.get_text(strip=True)))\n",
    "        \n",
    "        # Extract Traditional Chinese name (Hanja)\n",
    "        chinese_row = infobox.find('th', string=re.compile(r'Hanja', re.IGNORECASE))\n",
    "        if chinese_row:\n",
    "            chinese_data = chinese_row.find_next_sibling('td')\n",
    "            if chinese_data:\n",
    "                group_info['hanja'] = ''.join(clean_text_footnotes(chinese_data.get_text()).split())\n",
    "        \n",
    "        # Extract Hangul name\n",
    "        hangul_row = infobox.find('th', string=re.compile(r'Hangul', re.IGNORECASE))\n",
    "        if hangul_row:\n",
    "            hangul_data = hangul_row.find_next_sibling('td')\n",
    "            if hangul_data:\n",
    "                group_info['hangul'] = clean_text_footnotes(hangul_data.get_text(strip=True))\n",
    "        \n",
    "        # Extract members\n",
    "        members_row = infobox.find('th', string=re.compile(r'Members', re.IGNORECASE))\n",
    "        if members_row:\n",
    "            members_data = members_row.find_next_sibling('td')\n",
    "            if members_data:\n",
    "                members_list = [clean_text_footnotes(li.get_text(strip=True)) for li in members_data.find_all('li')]\n",
    "                group_info['members'] = members_list\n",
    "        \n",
    "        return group_info\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the Wikipedia page: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing the Infobox: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_year': 2020, 'end_year': None, 'company': 'SM', 'hanja': None, 'hangul': None, 'members': ['Karina', 'Giselle', 'Winter', 'Ningning']}\n",
      "{'start_year': 2007, 'end_year': None, 'company': 'SM', 'hanja': '少女時代', 'hangul': '소녀시대', 'members': ['Taeyeon', 'Sunny', 'Tiffany', 'Hyoyeon', 'Yuri', 'Sooyoung', 'Yoona', 'Seohyun']}\n",
      "{'start_year': 2006, 'end_year': None, 'company': 'YG', 'hanja': None, 'hangul': None, 'members': ['Taeyang', 'G-Dragon', 'Daesung']}\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(fetch_group_background('Aespa'))\n",
    "print(fetch_group_background('Girls\\' Generation')) # test 2 stage active years\n",
    "print(fetch_group_background('Big_Bang_(band)')) # test need redirct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing background info found. Starting fresh.\n",
      "Fetching Chakra_(group)...\n",
      "Fetching Papaya_(group)...\n",
      "Fetching UN_(band)...\n",
      "Fetching 5tion...\n",
      "Fetching Jewelry_(group)...\n",
      "Fetching JtL...\n",
      "Fetching K'Pop_(band)...\n",
      "Fetching Kiss_(South_Korean_group)...\n",
      "Fetching Milk_(South_Korean_group)...\n",
      "Fetching Black_Beat...\n",
      "Fetching F-iV...\n",
      "Fetching Isak_N_Jiyeon...\n",
      "Fetching Luv_(group)...\n",
      "Fetching MC_the_Max...\n",
      "Fetching Noel_(band)...\n",
      "Fetching Shinvi...\n",
      "Error parsing the Infobox: No infobox found on the page.\n",
      "Fetching Sugar_(South_Korean_group)...\n",
      "Fetching Brown_Eyed_Soul_(band)...\n",
      "Fetching Take_(band)...\n",
      "Fetching TVXQ...\n",
      "Fetching TraxX...\n",
      "Fetching V.O.S_(band)...\n",
      "Fetching Gavy_NJ...\n",
      "Fetching LPG_(South_Korean_group)...\n",
      "Fetching Paran_(band)...\n",
      "Fetching SS501...\n",
      "Fetching Super_Junior...\n",
      "Fetching The_Grace_(group)...\n",
      "Fetching 2NB...\n",
      "Fetching Big_Bang_(band)...\n",
      "Fetching Brown_Eyed_Girls...\n",
      "Fetching SeeYa...\n",
      "Fetching Super_Junior-K.R.Y....\n",
      "Fetching Untouchable_(band)...\n",
      "Fetching Baby_Vox_Re.V...\n",
      "Fetching Black_Pearl_(South_Korean_group)...\n",
      "Fetching F.T._Island...\n",
      "Fetching Girls'_Generation...\n",
      "Fetching Kara_(South_Korean_group)...\n",
      "Fetching Sunny_Hill...\n",
      "Fetching Super_Junior-T...\n",
      "Fetching Supernova_(South_Korean_group)...\n",
      "Fetching T-max...\n",
      "Fetching Tritops...\n",
      "Fetching Wonder_Girls...\n",
      "Fetching 2AM_(band)...\n",
      "Fetching 2PM...\n",
      "Fetching Davichi...\n",
      "Fetching Miss_S...\n",
      "Fetching Shinee...\n",
      "Fetching Super_Junior-H...\n",
      "Fetching Super_Junior-M...\n",
      "Fetching U-KISS...\n",
      "Fetching 2NE1...\n",
      "Fetching 4Minute...\n",
      "Fetching After_School_(group)...\n",
      "Fetching Highlight_(band)...\n",
      "Fetching CNBLUE...\n",
      "Fetching F(x)_(musical_group)...\n",
      "Fetching JQT_(group)...\n",
      "Fetching MBLAQ...\n",
      "Fetching Rainbow_(girl_group)...\n",
      "Fetching Secret_(South_Korean_group)...\n",
      "Fetching Shu-I...\n",
      "Error parsing the Infobox: No infobox found on the page.\n",
      "Fetching T-ara...\n",
      "Fetching Urban_Zakapa...\n",
      "Fetching Coed_School...\n",
      "Fetching DMTN...\n",
      "Fetching F.Cuz...\n",
      "Fetching Girl's_Day...\n",
      "Fetching GD_&_TOP...\n",
      "Fetching GP_Basic...\n",
      "Fetching Homme_(band)...\n",
      "Fetching Infinite_(group)...\n",
      "Fetching JYJ...\n",
      "Fetching Led_Apple...\n",
      "Fetching Miss_A...\n",
      "Fetching Nine_Muses_(group)...\n",
      "Fetching One_Way_(South_Korean_band)...\n",
      "Fetching Orange_Caramel...\n",
      "Fetching Rooftop_Moonlight...\n",
      "Fetching Rhythm_Power...\n",
      "Fetching Sistar...\n",
      "Fetching Standing_Egg...\n",
      "Fetching Teen_Top...\n",
      "Fetching Touch_(South_Korean_group)...\n",
      "Fetching The_Boss_(band)...\n",
      "Fetching ZE:A...\n",
      "Fetching AA_(band)...\n",
      "Fetching Apeace...\n",
      "Fetching Apink...\n",
      "Fetching B1A4...\n",
      "Fetching Blady...\n",
      "Fetching Block_B...\n",
      "Fetching Boyfriend_(band)...\n",
      "Fetching BB_Girls...\n",
      "Fetching C-REAL...\n",
      "Fetching Chocolat_(group)...\n",
      "Fetching Clover_(musical_trio)...\n",
      "Fetching Dal_Shabet...\n",
      "Fetching F-ve_Dolls...\n",
      "Fetching Geeks_(musical_duo)...\n",
      "Fetching Kim_Heechul_&_Kim_Jungmo...\n",
      "Fetching M.I.B_(band)...\n",
      "Fetching Myname...\n",
      "Fetching N-Sonic...\n",
      "Fetching N-Train...\n",
      "Fetching Rania_(band)...\n",
      "Fetching Sistar19...\n",
      "Fetching Stellar_(group)...\n",
      "Fetching Super_Junior-D&E...\n",
      "Fetching Trouble_Maker_(duo)...\n",
      "Fetching Ulala_Session...\n",
      "Fetching 100%_(band)...\n",
      "Fetching 15&...\n",
      "Fetching 24K_(band)...\n",
      "Fetching A.cian...\n",
      "Fetching A-Jax_(band)...\n",
      "Fetching A-Prince...\n",
      "Fetching AOA_(group)...\n",
      "Fetching B.A.P_(South_Korean_band)...\n",
      "Fetching Big_Star_(South_Korean_band)...\n",
      "Fetching BtoB_(band)...\n",
      "Fetching C-Clown...\n",
      "Fetching Crayon_Pop...\n",
      "Fetching Cross_Gene...\n",
      "Fetching D-Unit...\n",
      "Fetching EvoL...\n",
      "Fetching EXID...\n",
      "Fetching Exo...\n",
      "Fetching Fiestar...\n",
      "Fetching Gangkiz...\n",
      "Fetching Girls'_Generation-TTS...\n",
      "Fetching Glam_(group)...\n",
      "Fetching Hello_Venus...\n",
      "Fetching Honey_G_(band)...\n",
      "Fetching JJ_Project...\n",
      "Fetching Lunafly...\n",
      "Fetching Mr.Mr_(band)...\n",
      "Fetching NU'EST...\n",
      "Fetching Phantom_(band)...\n",
      "Fetching Puretty...\n",
      "Fetching She'z...\n",
      "Fetching Skarf...\n",
      "Fetching Spica_(group)...\n",
      "Fetching Sunny_Days_(group)...\n",
      "Fetching Tahiti_(group)...\n",
      "Fetching Tasty_(band)...\n",
      "Fetching The_SeeYa...\n",
      "Fetching Tiny-G...\n",
      "Fetching Two_X...\n",
      "Fetching VIXX...\n",
      "Fetching Wonder_Boyz...\n",
      "Fetching 2Eyes...\n",
      "Fetching 2Yoon...\n",
      "Fetching 5urprise...\n",
      "Fetching AlphaBat...\n",
      "Fetching AOA_Black...\n",
      "Fetching Bestie_(group)...\n",
      "Fetching Boys_Republic_(band)...\n",
      "Fetching BTS...\n",
      "Fetching GI_(group)...\n",
      "Fetching History_(band)...\n",
      "Fetching Infinite_H...\n",
      "Fetching Ladies'_Code...\n",
      "Fetching LC9_(band)...\n",
      "Fetching M.Pire...\n",
      "Fetching QBS_(band)...\n",
      "Fetching Royal_Pirates...\n",
      "Fetching Soohyun_&_Hoon...\n",
      "Fetching Speed_(South_Korean_band)...\n",
      "Fetching T-ara_N4...\n",
      "Fetching Xeno-T...\n",
      "Fetching Wassup_(band)...\n",
      "Fetching 2000_Won...\n",
      "Fetching 4L_(group)...\n",
      "Fetching 4Ten...\n",
      "Fetching AKMU...\n",
      "Fetching Almeng...\n",
      "Fetching B.I.G_(band)...\n",
      "Fetching Badkiz...\n",
      "Fetching Be.A...\n",
      "Fetching Beatwin...\n",
      "Fetching Berry_Good...\n",
      "Fetching Bigflo...\n",
      "Fetching Bob_Girls...\n",
      "Fetching Bursters...\n",
      "Fetching D.Holic...\n",
      "Fetching GD_X_Taeyang...\n",
      "Fetching Got7...\n",
      "Fetching HALO_(South_Korean_group)...\n",
      "Fetching HeartB...\n",
      "Fetching Hi_Suhyun...\n",
      "Fetching High4_(band)...\n",
      "Fetching Hotshot_(band)...\n",
      "Fetching Infinite_F...\n",
      "Fetching JJCC...\n",
      "Fetching Laboum...\n",
      "Fetching Lip_Service_(group)...\n",
      "Fetching Lovelyz...\n",
      "Fetching Madtown...\n",
      "Fetching Mamamoo...\n",
      "Fetching Melody_Day_(group)...\n",
      "Fetching Minx_(band)...\n",
      "Fetching Nasty_Nasty_(band)...\n",
      "Fetching Play_the_Siren...\n",
      "Fetching Red_Velvet_(group)...\n",
      "Fetching Sonamoo...\n",
      "Fetching Strawberry_Milk...\n",
      "Fetching The_Barberettes...\n",
      "Fetching The_Legend_(band)...\n",
      "Fetching Toheart_(band)...\n",
      "Fetching Troy_(band)...\n",
      "Fetching Uniq_(band)...\n",
      "Fetching Wings_(duo)...\n",
      "Fetching Winner_(band)...\n",
      "Fetching Year_7_Class_1...\n",
      "Fetching 1Punch...\n",
      "Fetching April_(band)...\n",
      "Fetching Bastarz...\n",
      "Fetching CLC_(group)...\n",
      "Fetching Day6...\n",
      "Fetching DIA_(group)...\n",
      "Fetching GFriend...\n",
      "Fetching IKon...\n",
      "Fetching MAP6_(band)...\n",
      "Fetching MAS_(band)...\n",
      "Fetching Monsta_X...\n",
      "Fetching MyB...\n",
      "Fetching N.Flying...\n",
      "Fetching Oh_My_Girl...\n",
      "Fetching Playback_(South_Korean_group)...\n",
      "Fetching Romeo_(band)...\n",
      "Fetching Rubber_Soul_(group)...\n",
      "Fetching Seventeen_(South_Korean_band)...\n",
      "Fetching Snuper...\n",
      "Fetching Twice...\n",
      "Fetching Unicorn_(South_Korean_group)...\n",
      "Fetching Up10tion...\n",
      "Fetching VAV_(band)...\n",
      "Fetching VIXX_LR...\n",
      "Fetching AOA_Cream...\n",
      "Fetching Astro_(South_Korean_band)...\n",
      "Fetching Blackpink...\n",
      "Fetching Bolbbalgan4...\n",
      "Fetching Boys24...\n",
      "Fetching BtoB_Blue...\n",
      "Fetching CocoSori...\n",
      "Fetching Double_S_301...\n",
      "Fetching Exo-CBX...\n",
      "Fetching Gugudan...\n",
      "Fetching I.B.I...\n",
      "Fetching I.O.I...\n",
      "Fetching Imfact...\n",
      "Fetching KNK_(band)...\n",
      "Fetching MASC_(band)...\n",
      "Fetching MOBB...\n",
      "Fetching Momoland...\n",
      "Fetching NCT_(group)...\n",
      "Fetching NCT_U...\n",
      "Fetching NCT_127...\n",
      "Fetching NCT_Dream...\n",
      "Fetching Nine_Muses_A...\n",
      "Fetching Pentagon_(South_Korean_band)...\n",
      "Fetching SF9...\n",
      "Fetching The_East_Light...\n",
      "Fetching Unnies...\n",
      "Fetching Victon...\n",
      "Fetching Vromance...\n",
      "Fetching Cosmic_Girls...\n",
      "Fetching 14U...\n",
      "Fetching Stray_Kids...\n",
      "Fetching A.C.E_(South_Korean_band)...\n",
      "Fetching Boy_Story...\n",
      "Fetching Busters_(group)...\n",
      "Fetching Dreamcatcher_(group)...\n",
      "Fetching Duetto_(duo)...\n",
      "Fetching Elris...\n",
      "Fetching Favorite_(group)...\n",
      "Fetching Golden_Child_(band)...\n",
      "Fetching Good_Day_(band)...\n",
      "Fetching GreatGuys...\n",
      "Fetching HashTag_(group)...\n",
      "Fetching Honeyst...\n",
      "Fetching Hyeongseop_X_Euiwoong...\n",
      "Fetching In2It...\n",
      "Fetching IZ_(band)...\n",
      "Fetching JBJ_(band)...\n",
      "Fetching Kard_(band)...\n",
      "Fetching Longguo_&_Shihyun...\n",
      "Fetching M.O.N.T...\n",
      "Fetching Mind_U...\n",
      "Fetching MVP_(South_Korean_band)...\n",
      "Fetching MXM_(musical_duo)...\n",
      "Fetching Myteen...\n",
      "Fetching NU'EST_W...\n",
      "Fetching ONF_(band)...\n",
      "Fetching P.O.P_(band)...\n",
      "Fetching Pristin...\n",
      "Fetching Rainz_(band)...\n",
      "Fetching S.I.S_(group)...\n",
      "Fetching Seven_O'Clock...\n",
      "Fetching The_Boyz_(South_Korean_band)...\n",
      "Fetching The_Rose_(band)...\n",
      "Fetching TRCNG...\n",
      "Fetching Triple_H_(band)...\n",
      "Fetching TST_(band)...\n",
      "Fetching Varsity_(group)...\n",
      "Fetching Wanna_One...\n",
      "Fetching Weki_Meki...\n",
      "Fetching A_Train_To_Autumn...\n",
      "Fetching Ateez...\n",
      "Fetching D-Crunch...\n",
      "Fetching Dream_Note...\n",
      "Fetching Fromis_9...\n",
      "Fetching (G)I-dle...\n",
      "Fetching Girlkind...\n",
      "Fetching Gugudan_SeMiNa...\n",
      "Fetching GWSN...\n",
      "Fetching Honey_Popcorn...\n",
      "Fetching Iz*One...\n",
      "Fetching JBJ95...\n",
      "Fetching DA...\n",
      "Error parsing the Infobox: No infobox found on the page.\n",
      "Fetching Loona...\n",
      "Fetching Maywish...\n",
      "Fetching Nature_(band)...\n",
      "Fetching NeonPunch...\n",
      "Fetching Noir_(band)...\n",
      "Fetching NTB_(band)...\n",
      "Fetching Oh!GG...\n",
      "Fetching Pink_Fantasy...\n",
      "Fetching Pristin_V...\n",
      "Fetching Saturday_(group)...\n",
      "Fetching Spectrum_(group)...\n",
      "Fetching Target_(South_Korean_band)...\n",
      "Fetching UNB_(group)...\n",
      "Fetching Uni.T...\n",
      "Fetching W24_(band)...\n",
      "Fetching We_Girls...\n",
      "Fetching WJMK_(band)...\n",
      "Fetching 1Team...\n",
      "Fetching 1the9...\n",
      "Fetching 3YE...\n",
      "Fetching AB6IX...\n",
      "Fetching Ariaz...\n",
      "Fetching Argon_(band)...\n",
      "Fetching BDC_(group)...\n",
      "Fetching Bvndit...\n",
      "Fetching Cherry_Bullet...\n",
      "Fetching CIX_(band)...\n",
      "Fetching D1CE...\n",
      "Fetching DKZ...\n",
      "Fetching ENOi...\n",
      "Fetching Everglow...\n",
      "Fetching Exo-SC...\n",
      "Fetching Fanatics_(group)...\n",
      "Fetching Hinapia...\n",
      "Fetching Hoppipolla_(band)...\n",
      "Fetching Itzy...\n",
      "Fetching Jus2...\n",
      "Fetching MustB...\n",
      "Fetching Newkidd...\n",
      "Fetching Oneus...\n",
      "Fetching Onewe...\n",
      "Fetching OnlyOneOf...\n",
      "Fetching Purple_Rain_(band)...\n",
      "Fetching Purplebeck...\n",
      "Fetching Rocket_Punch...\n",
      "Fetching SuperM...\n",
      "Fetching Teen_Teen...\n",
      "Fetching Tomorrow_X_Together...\n",
      "Fetching Vanner_(band)...\n",
      "Fetching Verivery...\n",
      "Fetching We_in_the_Zone...\n",
      "Fetching Wooseok_x_Kuanlin...\n",
      "Fetching X1_(band)...\n",
      "Fetching Aespa...\n",
      "Fetching Moonbin_&_Sanha...\n",
      "Fetching B.O.Y...\n",
      "Fetching BAE173...\n",
      "Fetching Blackswan...\n",
      "Fetching Botopass...\n",
      "Fetching BtoB_4U...\n",
      "Fetching Cignature...\n",
      "Fetching Cravity...\n",
      "Fetching Craxy...\n",
      "Fetching DKB_(band)...\n",
      "Fetching Drippin...\n",
      "Fetching E'Last...\n",
      "Fetching Enhypen...\n",
      "Fetching Day6_(Even_of_Day)...\n",
      "Fetching Ghost9...\n",
      "Fetching H&D...\n",
      "Fetching Lunarsolar...\n",
      "Fetching MCND...\n",
      "Fetching NiziU...\n",
      "Fetching P1Harmony...\n",
      "Fetching Redsquare...\n",
      "Fetching Red_Velvet_–_Irene_&_Seulgi...\n",
      "Fetching Refund_Sisters...\n",
      "Fetching Secret_Number...\n",
      "Fetching SSAK3...\n",
      "Fetching STAYC...\n",
      "Fetching TO1...\n",
      "Fetching Treasure_(band)...\n",
      "Fetching UNVS...\n",
      "Fetching Weeekly...\n",
      "Fetching WEi...\n",
      "Fetching Wooah...\n",
      "Fetching Billlie...\n",
      "Fetching Blitzers...\n",
      "Fetching BugAboo...\n",
      "Fetching Ciipher...\n",
      "Fetching Epex...\n",
      "Fetching Hot_Issue_(group)...\n",
      "Fetching Ichillin'...\n",
      "Fetching Ive_(group)...\n",
      "Fetching Just_B...\n",
      "Fetching Lightsum...\n",
      "Fetching Luminous_(group)...\n",
      "Fetching Mirae_(band)...\n",
      "Fetching NTX_(group)...\n",
      "Fetching Omega_X...\n",
      "Fetching Pixy_(group)...\n",
      "Fetching Purple_Kiss...\n",
      "Fetching TFN_(group)...\n",
      "Fetching The_KingDom...\n",
      "Fetching Tri.be...\n",
      "Fetching Xdinary_Heroes...\n",
      "Fetching &Team...\n",
      "Fetching Acid_Angel_from_Asia...\n",
      "Fetching Aimers_(band)...\n",
      "Fetching Jinjin_&_Rocky...\n",
      "Fetching ATBO...\n",
      "Fetching Blank2y...\n",
      "Fetching Classy_(group)...\n",
      "Fetching CSR_(group)...\n",
      "Fetching Fifty_Fifty_(group)...\n",
      "Fetching Got_the_Beat...\n",
      "Fetching H1-Key...\n",
      "Fetching ILY:1...\n",
      "Fetching Irris...\n",
      "Fetching Kep1er...\n",
      "Fetching Lapillus_(group)...\n",
      "Fetching Le_Sserafim...\n",
      "Fetching Mamamoo+...\n",
      "Fetching Mimiirose...\n",
      "Fetching NewJeans...\n",
      "Fetching Nmixx...\n",
      "Fetching TAN_(group)...\n",
      "Fetching Tempest_(South_Korean_band)...\n",
      "Fetching The_New_Six...\n",
      "Fetching Trendz_(group)...\n",
      "Fetching Viviz...\n",
      "Fetching WSG_Wannabe...\n",
      "Fetching Younite...\n",
      "Fetching 8Turn...\n",
      "Fetching 82Major...\n",
      "Fetching Ampers&One...\n",
      "Fetching Babymonster...\n",
      "Fetching BoyNextDoor...\n",
      "Fetching BXB_(group)...\n",
      "Fetching El7z_Up...\n",
      "Fetching Evnne...\n",
      "Fetching Fantasy_Boys_(group)...\n",
      "Fetching Hori7on...\n",
      "Fetching Kiss_of_Life_(group)...\n",
      "Fetching Loossemble...\n",
      "Fetching Lun8...\n",
      "Fetching Mave:...\n",
      "Fetching N.SSign...\n",
      "Fetching NCT_DoJaeJung...\n",
      "Fetching One_Pact...\n",
      "Fetching Plave_(band)...\n",
      "Fetching Pow_(band)...\n",
      "Fetching Primrose_(group)...\n",
      "Fetching Riize...\n",
      "Fetching Shownu_X_Hyungwon...\n",
      "Fetching The_Wind_(South_Korean_band)...\n",
      "Fetching TIOT...\n",
      "Fetching TripleS...\n",
      "Fetching Xikers...\n",
      "Fetching Xodiac_(band)...\n",
      "Fetching Young_Posse...\n",
      "Fetching Zerobaseone...\n",
      "Fetching All(H)Ours...\n",
      "Fetching ARrC...\n",
      "Fetching Artms...\n",
      "Fetching B.D.U...\n",
      "Fetching Badvillain...\n",
      "Fetching Big_Ocean...\n",
      "Fetching Candy_Shop_(group)...\n",
      "Fetching Dragon_Pony...\n",
      "Fetching Dxmon...\n",
      "Fetching Geenius...\n",
      "Fetching Illit...\n",
      "Fetching Izna...\n",
      "Fetching Madein...\n",
      "Fetching Meovv...\n",
      "Fetching Mytro...\n",
      "Fetching NOMAD_(group)...\n",
      "Fetching Nowadays_(group)...\n",
      "Fetching Rescene...\n",
      "Fetching Say_My_Name_(group)...\n",
      "Fetching TWS_(group)...\n",
      "Fetching Unis_(group)...\n",
      "Fetching Waker...\n",
      "Fetching Waterfire_(band)...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "background_csv_path = 'background_info.csv'\n",
    "background_info_list = []\n",
    "\n",
    "try:\n",
    "    existing_background_df = pd.read_csv(background_csv_path)\n",
    "    processed_groups = set(existing_background_df['group_wiki_name'])\n",
    "    background_info_list = existing_background_df.to_dict('records')\n",
    "    print(f\"Loaded existing background info for {len(processed_groups)} groups.\")\n",
    "except FileNotFoundError:\n",
    "    processed_groups = set()\n",
    "    background_info_list = []\n",
    "    print(\"No existing background info found. Starting fresh.\")\n",
    "\n",
    "filtered_df = all_decades_df[all_decades_df['start_year'] >= 2000].copy()\n",
    "\n",
    "for index, row in filtered_df.iterrows():\n",
    "    group_name = row['group_wiki_name']\n",
    "    if group_name in processed_groups:\n",
    "        continue\n",
    "    \n",
    "    print(f'Fetching {group_name}...')\n",
    "    try:\n",
    "        background = fetch_group_background(group_name)\n",
    "        \n",
    "        group_background = {\n",
    "            'group_wiki_name': group_name,\n",
    "            'bg_start_year': background.get('start_year'),\n",
    "            'bg_end_year': background.get('end_year'),\n",
    "            'bg_company': background.get('company'),\n",
    "            'bg_hanja': background.get('hanja'),\n",
    "            'bg_hangul': background.get('hangul'),\n",
    "            'bg_members': background.get('members'),\n",
    "        }\n",
    "        background_info_list.append(group_background)\n",
    "        \n",
    "        pd.DataFrame([group_background]).to_csv(\n",
    "            background_csv_path, mode='a', index=False, header=not processed_groups\n",
    "        )\n",
    "        processed_groups.add(group_name)\n",
    "        \n",
    "        # time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching background for {group_name}: {e}\")\n",
    "\n",
    "\n",
    "background_df = pd.DataFrame(background_info_list)\n",
    "group_info_df = pd.merge(filtered_df, background_df, on='group_wiki_name', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_year</th>\n",
       "      <th>group</th>\n",
       "      <th>wiki_url</th>\n",
       "      <th>decade</th>\n",
       "      <th>group_wiki_name</th>\n",
       "      <th>bg_start_year</th>\n",
       "      <th>bg_end_year</th>\n",
       "      <th>bg_company</th>\n",
       "      <th>bg_hanja</th>\n",
       "      <th>bg_hangul</th>\n",
       "      <th>bg_members</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>Chakra</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Chakra_(group)</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Chakra_(group)</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Hwangbo, Bona, Eun, Ryeowon, Eani]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>Papaya</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Papaya_(group)</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Papaya_(group)</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Kang Kyoung-ah, Joo Yeun-jung, Cho Hye-kyung,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>UN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/UN_(band)</td>\n",
       "      <td>2000s</td>\n",
       "      <td>UN_(band)</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>NH Planning</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>5tion</td>\n",
       "      <td>https://en.wikipedia.org/wiki/5tion</td>\n",
       "      <td>2000s</td>\n",
       "      <td>5tion</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Il Kwon, Chang Woo, Ju Ho, Jun Young, Jun Ho]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jewelry_(group)</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Jewelry_(group)</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Star Empire</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Kim Ye-won, Baby J, Kim Eunjung, Jung Yoo-jin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>2024</td>\n",
       "      <td>Say My Name</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Say_My_Name_(group)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Say_My_Name_(group)</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Hitomi, Mei, Kanny, Soha, Dohee, Junhwi, Seun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>2024</td>\n",
       "      <td>TWS</td>\n",
       "      <td>https://en.wikipedia.org/wiki/TWS_(group)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>TWS_(group)</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pledis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Shinyu, Dohoon, Youngjae, Hanjin, Jihoon, Kyu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>2024</td>\n",
       "      <td>Unis</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Unis_(group)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Unis_(group)</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Hyeonju, Nana, Gehlee, Kotoko, Yunha, Elisia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>2024</td>\n",
       "      <td>Waker</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Waker</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Waker</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Kohyeon, Kwon Hyeop, Ijun, Leo, Saebyeol, Sebum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>2024</td>\n",
       "      <td>Waterfire</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Waterfire_(band)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Waterfire_(band)</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Sunyoul, Wumuti, Choi Su-hwan, Kang Ha-yoon]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     start_year        group  \\\n",
       "0          2000       Chakra   \n",
       "1          2000       Papaya   \n",
       "2          2000           UN   \n",
       "3          2001        5tion   \n",
       "4          2001      Jewelry   \n",
       "..          ...          ...   \n",
       "507        2024  Say My Name   \n",
       "508        2024          TWS   \n",
       "509        2024         Unis   \n",
       "510        2024        Waker   \n",
       "511        2024    Waterfire   \n",
       "\n",
       "                                              wiki_url decade  \\\n",
       "0         https://en.wikipedia.org/wiki/Chakra_(group)  2000s   \n",
       "1         https://en.wikipedia.org/wiki/Papaya_(group)  2000s   \n",
       "2              https://en.wikipedia.org/wiki/UN_(band)  2000s   \n",
       "3                  https://en.wikipedia.org/wiki/5tion  2000s   \n",
       "4        https://en.wikipedia.org/wiki/Jewelry_(group)  2000s   \n",
       "..                                                 ...    ...   \n",
       "507  https://en.wikipedia.org/wiki/Say_My_Name_(group)  2020s   \n",
       "508          https://en.wikipedia.org/wiki/TWS_(group)  2020s   \n",
       "509         https://en.wikipedia.org/wiki/Unis_(group)  2020s   \n",
       "510                https://en.wikipedia.org/wiki/Waker  2020s   \n",
       "511     https://en.wikipedia.org/wiki/Waterfire_(band)  2020s   \n",
       "\n",
       "         group_wiki_name  bg_start_year  bg_end_year   bg_company bg_hanja  \\\n",
       "0         Chakra_(group)         2000.0       2006.0         None     None   \n",
       "1         Papaya_(group)         2000.0          NaN         None     None   \n",
       "2              UN_(band)         2000.0       2005.0  NH Planning     None   \n",
       "3                  5tion         2001.0          NaN         None     None   \n",
       "4        Jewelry_(group)         2001.0          NaN  Star Empire     None   \n",
       "..                   ...            ...          ...          ...      ...   \n",
       "507  Say_My_Name_(group)         2024.0          NaN         None     None   \n",
       "508          TWS_(group)         2024.0          NaN       Pledis     None   \n",
       "509         Unis_(group)         2024.0          NaN         None     None   \n",
       "510                Waker         2024.0          NaN         None     None   \n",
       "511     Waterfire_(band)         2024.0          NaN                  None   \n",
       "\n",
       "    bg_hangul                                         bg_members  \n",
       "0        None                [Hwangbo, Bona, Eun, Ryeowon, Eani]  \n",
       "1        None  [Kang Kyoung-ah, Joo Yeun-jung, Cho Hye-kyung,...  \n",
       "2        None                                                 []  \n",
       "3        None     [Il Kwon, Chang Woo, Ju Ho, Jun Young, Jun Ho]  \n",
       "4        None  [Kim Ye-won, Baby J, Kim Eunjung, Jung Yoo-jin...  \n",
       "..        ...                                                ...  \n",
       "507      None  [Hitomi, Mei, Kanny, Soha, Dohee, Junhwi, Seun...  \n",
       "508      None  [Shinyu, Dohoon, Youngjae, Hanjin, Jihoon, Kyu...  \n",
       "509      None  [Hyeonju, Nana, Gehlee, Kotoko, Yunha, Elisia,...  \n",
       "510      None  [Kohyeon, Kwon Hyeop, Ijun, Leo, Saebyeol, Sebum]  \n",
       "511      None      [Sunyoul, Wumuti, Choi Su-hwan, Kang Ha-yoon]  \n",
       "\n",
       "[512 rows x 11 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_year</th>\n",
       "      <th>group</th>\n",
       "      <th>wiki_url</th>\n",
       "      <th>decade</th>\n",
       "      <th>group_wiki_name</th>\n",
       "      <th>bg_start_year</th>\n",
       "      <th>bg_end_year</th>\n",
       "      <th>bg_company</th>\n",
       "      <th>bg_hanja</th>\n",
       "      <th>bg_hangul</th>\n",
       "      <th>bg_members</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>Chakra</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Chakra_(group)</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Chakra_(group)</td>\n",
       "      <td>2000</td>\n",
       "      <td>2006</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Hwangbo, Bona, Eun, Ryeowon, Eani]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>Papaya</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Papaya_(group)</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Papaya_(group)</td>\n",
       "      <td>2000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Kang Kyoung-ah, Joo Yeun-jung, Cho Hye-kyung,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>UN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/UN_(band)</td>\n",
       "      <td>2000s</td>\n",
       "      <td>UN_(band)</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>NH Planning</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>5tion</td>\n",
       "      <td>https://en.wikipedia.org/wiki/5tion</td>\n",
       "      <td>2000s</td>\n",
       "      <td>5tion</td>\n",
       "      <td>2001</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Il Kwon, Chang Woo, Ju Ho, Jun Young, Jun Ho]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jewelry_(group)</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Jewelry_(group)</td>\n",
       "      <td>2001</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Star Empire</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Kim Ye-won, Baby J, Kim Eunjung, Jung Yoo-jin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>2024</td>\n",
       "      <td>Say My Name</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Say_My_Name_(group)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Say_My_Name_(group)</td>\n",
       "      <td>2024</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Hitomi, Mei, Kanny, Soha, Dohee, Junhwi, Seun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>2024</td>\n",
       "      <td>TWS</td>\n",
       "      <td>https://en.wikipedia.org/wiki/TWS_(group)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>TWS_(group)</td>\n",
       "      <td>2024</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Pledis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Shinyu, Dohoon, Youngjae, Hanjin, Jihoon, Kyu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>2024</td>\n",
       "      <td>Unis</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Unis_(group)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Unis_(group)</td>\n",
       "      <td>2024</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Hyeonju, Nana, Gehlee, Kotoko, Yunha, Elisia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>2024</td>\n",
       "      <td>Waker</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Waker</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Waker</td>\n",
       "      <td>2024</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Kohyeon, Kwon Hyeop, Ijun, Leo, Saebyeol, Sebum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>2024</td>\n",
       "      <td>Waterfire</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Waterfire_(band)</td>\n",
       "      <td>2020s</td>\n",
       "      <td>Waterfire_(band)</td>\n",
       "      <td>2024</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Sunyoul, Wumuti, Choi Su-hwan, Kang Ha-yoon]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     start_year        group  \\\n",
       "0          2000       Chakra   \n",
       "1          2000       Papaya   \n",
       "2          2000           UN   \n",
       "3          2001        5tion   \n",
       "4          2001      Jewelry   \n",
       "..          ...          ...   \n",
       "507        2024  Say My Name   \n",
       "508        2024          TWS   \n",
       "509        2024         Unis   \n",
       "510        2024        Waker   \n",
       "511        2024    Waterfire   \n",
       "\n",
       "                                              wiki_url decade  \\\n",
       "0         https://en.wikipedia.org/wiki/Chakra_(group)  2000s   \n",
       "1         https://en.wikipedia.org/wiki/Papaya_(group)  2000s   \n",
       "2              https://en.wikipedia.org/wiki/UN_(band)  2000s   \n",
       "3                  https://en.wikipedia.org/wiki/5tion  2000s   \n",
       "4        https://en.wikipedia.org/wiki/Jewelry_(group)  2000s   \n",
       "..                                                 ...    ...   \n",
       "507  https://en.wikipedia.org/wiki/Say_My_Name_(group)  2020s   \n",
       "508          https://en.wikipedia.org/wiki/TWS_(group)  2020s   \n",
       "509         https://en.wikipedia.org/wiki/Unis_(group)  2020s   \n",
       "510                https://en.wikipedia.org/wiki/Waker  2020s   \n",
       "511     https://en.wikipedia.org/wiki/Waterfire_(band)  2020s   \n",
       "\n",
       "         group_wiki_name  bg_start_year  bg_end_year   bg_company bg_hanja  \\\n",
       "0         Chakra_(group)           2000         2006         None     None   \n",
       "1         Papaya_(group)           2000         <NA>         None     None   \n",
       "2              UN_(band)           2000         2005  NH Planning     None   \n",
       "3                  5tion           2001         <NA>         None     None   \n",
       "4        Jewelry_(group)           2001         <NA>  Star Empire     None   \n",
       "..                   ...            ...          ...          ...      ...   \n",
       "507  Say_My_Name_(group)           2024         <NA>         None     None   \n",
       "508          TWS_(group)           2024         <NA>       Pledis     None   \n",
       "509         Unis_(group)           2024         <NA>         None     None   \n",
       "510                Waker           2024         <NA>         None     None   \n",
       "511     Waterfire_(band)           2024         <NA>                  None   \n",
       "\n",
       "    bg_hangul                                         bg_members  \n",
       "0        None                [Hwangbo, Bona, Eun, Ryeowon, Eani]  \n",
       "1        None  [Kang Kyoung-ah, Joo Yeun-jung, Cho Hye-kyung,...  \n",
       "2        None                                                 []  \n",
       "3        None     [Il Kwon, Chang Woo, Ju Ho, Jun Young, Jun Ho]  \n",
       "4        None  [Kim Ye-won, Baby J, Kim Eunjung, Jung Yoo-jin...  \n",
       "..        ...                                                ...  \n",
       "507      None  [Hitomi, Mei, Kanny, Soha, Dohee, Junhwi, Seun...  \n",
       "508      None  [Shinyu, Dohoon, Youngjae, Hanjin, Jihoon, Kyu...  \n",
       "509      None  [Hyeonju, Nana, Gehlee, Kotoko, Yunha, Elisia,...  \n",
       "510      None  [Kohyeon, Kwon Hyeop, Ijun, Leo, Saebyeol, Sebum]  \n",
       "511      None      [Sunyoul, Wumuti, Choi Su-hwan, Kang Ha-yoon]  \n",
       "\n",
       "[512 rows x 11 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_info_df['bg_start_year'] = pd.to_numeric(group_info_df['bg_start_year'], errors='coerce').astype('Int64')\n",
    "group_info_df['bg_end_year'] = pd.to_numeric(group_info_df['bg_end_year'], errors='coerce').astype('Int64')\n",
    "group_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info_df.to_csv('./data/group_info_from_2000.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
